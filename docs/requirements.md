# Требования к "Информационному радару"

## Цель проекта

Создать консольное приложение "Информационный радар", которое помогает пользователю эффективно управлять потоком технических статей из различных источников, автоматизируя сбор данных и предоставляя удобный интерфейс для разбора и сохранения интересного контента.

---

## Фаза 1: Сбор данных

### Функциональные требования (ФТ)

*   **FT1.1: Конфигурация источников** Приложение должно загружать конфигурацию источников (например, хабы Habr) из файла `config.yml`.
    *Пример структуры `config.yml` для Habr:*
    ```yaml
    habr:
      # Список интересующих хабов (используется "слаг" из URL)
      hubs:
        - artificial_intelligence
    ```
*   **FT1.2: Гибридный сбор данных** Для каждого настроенного источника (например, Habr) приложение должно:
    *   **FT1.2.1:** Обнаруживать новые статьи, используя RSS-фиды.
    *   **FT1.2.2:** Определять "разрывы" в RSS-ленте (когда самая старая статья в RSS новее, чем последняя известная в БД).
    *   **FT1.2.3:** При обнаружении разрыва, выполнять постраничный скрапинг списка статей хаба до тех пор, пока не будут найдены статьи, уже присутствующие в БД, или статьи, опубликованные до даты последней известной статьи.
    *   **FT1.2.4:** Обогащать каждую новую статью, скрапя её HTML-страницу для извлечения дополнительных метаданных (рейтинг, просмотры, время чтения, комментарии, теги).
*   **FT1.3: Хранение собранных данных** Приложение должно сохранять собранные статьи в локальной базе данных SQLite (`inforadar.db`):
    *   **FT1.3.1:** Каждая статья должна иметь уникальный идентификатор (GUID) для предотвращения дубликатов.
    *   **FT1.3.2:** Основные метаданные (заголовок, идентификатор, ссылка, описание, дата публикации, автор) должны храниться в отдельных колонках.
    *   **FT1.3.3:** Дополнительные метаданные должны храниться в JSON-колонке. Для источника Habr это: уровень сложности, время на прочтение, количество просмотров, тэги, хабы, рейтинг, количество комментариев.

### Нефункциональные требования (НФТ)

*   **NFT1.1: Этичный скрапинг** Процесс сбора данных должен быть "вежливым" по отношению к источникам:
    *   **NFT1.1.1:** Использовать `User-Agent`, имитирующий браузер.
    *   **NFT1.1.2:** Вставлять случайные паузы между HTTP-запросами для предотвращения чрезмерной нагрузки на серверы источников и снижения риска блокировки IP-адреса.
    *   **NFT1.1.3:** Корректно обрабатывать ошибки сети и парсинга, не прерывая весь процесс сбора.
*   **NFT1.2: Устойчивость к изменениям структуры** Скрапинг должен быть максимально устойчив к незначительным изменениям в HTML-структуре страниц источников (например, использование нескольких селекторов для одного элемента).
